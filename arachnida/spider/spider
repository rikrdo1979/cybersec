#!/usr/bin/env python
# -*- coding: utf-8 -*-

__author__ = "rikrdo"
__copyright__ = "Copyright 2022, Bootcamp Cybersec "
__credits__ = ["rikrdo"]
__license__ = "GPL"
__version__ = "1.0.1"
__maintainer__ = "rikrdo"
__email__ = "rikrdo@rikrdo.es"
__status__ = "Production"

print('''
                   .__    .___              
      ____________ |__| __| _/___________ 
     /  ___/\____ \|  |/ __ |/ __ \_  __ \\
     \___ \ |  |_> >  / /_/ \  ___/|  | \/
    /____  >|   __/|__\____ |\___  >__|   
         \/ |__|           \/    \/       
''')

# required libraries

import argparse, requests, re, os, urllib, random
from genericpath import exists
from urllib.parse import urlparse
from urllib.parse import urljoin
from tqdm import tqdm
from requests_html import HTMLSession
from time import sleep

session = HTMLSession()

headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:82.0) Gecko/20100101 Firefox/82.0' }

# check if the URL is reacheable and valid
def uri_validator(domain):
    try:
        result = requests.get(domain)
        return (result)
    except:
        return False

def save_img(src, dirpath):
    print("\n\n --- Images --- \n\n")
    if not os.path.isdir(dirpath):
        print(' \033[93mThe directory is not present. Creating a new one.\033[0m')
        os.mkdir(dirpath)
    for img in src:
        try:
            if uri_validator(img): 
                filename = img.split("/")[-1]
                ext = img.split(".")[-1]
                response = requests.get(img)
                # check if file exists
                if not exists(dirpath+filename):
                    # check if name is to long and replace for a shorter random number
                    if len(filename) > 128:
                       filename = str((random.randrange(100000, 9999999, 1)))+'.'+ext
                    if ext == "pdf" or ext == "docx":
                        media = " \033[91m["+ ext +"] \033[0m " + filename + " "
                    else:
                       media = " \033[96m["+ ext +"] \033[0m " + filename + " "
                    for i in tqdm(range(1), desc=f'{media}'):
                        file = open((dirpath+filename), 'wb')
                        file.write(response.content)
                        file.close()
                    print('\n')
                else:
                    print(" \033[91m["+ ext +"] \033[0m " + filename + " \033[91mFile Exists!\033[0m\n")
        except requests.exceptions.RequestException as e:
            print("\033[91m[error] \033[0m ", e.__dict__)

def find_links(crawl, i, dirpath, download):
    i = int(i)
    to_set = set()
    im_set = set()
    im_list = []

    while i > 0:
        # get all the working links
        for lines in crawl:
            try:
                # check if the URl is correct and in case of redirection starts to use the redirect one
                respond = requests.get(lines)
                lines = respond.url
                # save lines in the respond
                r = session.get(lines)
                # loop over the found links
                for url in r.html.links:
                    # replace spaces
                    url = url.replace(" ", "%20")
                    url_raw = urlparse(url)
                    # get relative path URL's
                    uri = lines.split('/')[:3]
                    if url.startswith('http') or url.startswith('/'):
                        if url_raw.path.startswith('/'):
                            url = ("/".join(uri))+url_raw.path
                        else:
                            url = ("/".join(uri))+"/"+url_raw.path
                        # save in a set to remove duplicates
                        to_set.add(url)
                        # convert set to a list to work with it
                        crawl = list(to_set)
            except requests.exceptions.RequestException as e:
                continue 
        # get all the working links for the images 
        for urls in crawl:
            print("\033[1;32m[url] \033[0m " + urls)
            if download:
                try:
                    response = urllib.request.urlopen(urls)
                    # get the ltd to join with relative path images
                    proto = urls.split('/')[:1]
                    uri_img = urls.split('/')[:3]
                    # filter the files with extensions in the list
                    for images in re.findall(':?([^"]*\.(?:png|jpeg|jpg|gif|bmp|pdf|docx))', str(response.read())):
                        if images.startswith('//'):
                            images = str(proto[0])+images
                        elif images[0] == ('/') and images[1] != '/':
                            images = ("/".join(uri_img))+images
                        elif images.startswith('http'):
                            images = images
                            # create a set to remove duplicates
                            im_set.add(images)
                            im_list = list(im_set)
                        else:
                            pass
                except urllib.error.HTTPError as e:
                    continue
                except urllib.error.URLError as e:
                    continue
        # check if we have to download images
        if download:
            save_img(im_list, dirpath)
        print("\n\n --- \n\n")
        i = i -1
    else:
        return(crawl)

# pretty script end after ctrl + c

def exit_gracefully():
    print('\033[91m  ------------------------     @@@ Bye Bye!!! @@@  ------------------------\033[0m')

# collect arguments:
# -r = True/False
# -l = deepness level (default 5)
# -p = path (default "./data/")

def main():
    parser = argparse.ArgumentParser()
    parser.version = '1.0.1'
    parser.add_argument('URL', type=str, help='URL is required')
    parser.add_argument('-r', action='store_true', help='download recursively all the images from the given URL')
    parser.add_argument('-l', default = 5, help='set level of recursion')
    parser.add_argument('-p',default = './data/', help='decide the path to be save downloaded files')

    args = parser.parse_args()
    domain = args.URL
    dirpath = args.p
    download = args.r

    if args.l:
        if int(args.l) > 5:
            i = 5
        else:
            i = args.l
    else:
        i = 5

    if uri_validator(domain) != False:   

        print("\n \033[93m---------------------------------------------------------------- \033[0m ")
        print (" URL:\t\033[1m"+ domain + "\033[0m")
        print (" Deep:\t\033[1m"+ str(i) + "\033[0m")
        print (" Path:\t\033[1m"+ dirpath + "\033[0m")
        print (" Down:\t\033[1m"+ str(download) + "\033[0m")
        print(" \033[93m---------------------------------------------------------------- \033[0m \n")

        find_links([domain], i, dirpath, download)
    else:
        print('\n\033[91m ',domain, ':\033[0m Invalid URL, try again!\n')

if __name__=="__main__":
    try:
        main()
    except KeyboardInterrupt:
        pass
    finally:
        exit_gracefully()