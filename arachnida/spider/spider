#!/usr/bin/env python
# -*- coding: utf-8 -*-

__author__ = "rikrdo"
__copyright__ = "Copyright 2022, Bootcamp Cybersec "
__credits__ = ["rikrdo"]
__license__ = "GPL"
__version__ = "1.0.1"
__maintainer__ = "rikrdo"
__email__ = "rikrdo@rikrdo.es"
__status__ = "Production"

print('''
                   .__    .___              
      ____________ |__| __| _/___________ 
     /  ___/\____ \|  |/ __ |/ __ \_  __ \\
     \___ \ |  |_> >  / /_/ \  ___/|  | \/
    /____  >|   __/|__\____ |\___  >__|   
         \/ |__|           \/    \/       
''')

# required libraries

import argparse, requests, re, os, urllib, random, sys
from copy import deepcopy
from itertools import cycle
from genericpath import exists
from urllib.parse import urlparse
from urllib.parse import urljoin
from tqdm import tqdm
from requests_html import HTMLSession
from time import sleep

#animation = ['|', '/', '-', '\\'] # range 1,4
animation = ["⢿", "⣻", "⣽", "⣾", "⣷", "⣯", "⣟", "⡿"] # range 1,8
#animation = ["■         ","■■        ", "■■■       ", "■■■■      ", "■■■■■     ", "■■■■■■    ", "■■■■■■■   ", "■■■■■■■■  ", "■■■■■■■■■ ", "■■■■■■■■■■"] # range 1,10

session = HTMLSession()

headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:82.0) Gecko/20100101 Firefox/82.0' }

def error_log(e):
    try:
        f = open('log.txt', 'w')
        f.write('An exceptional thing happed - %s' % e)
        f.close()
    except:
        print("Fuck!!!!")
        pass

# check if the URL is reacheable and valid
def uri_validator(domain):
    try:
        result = requests.get(domain)
        return (result)
    except:
        return False

def save_img(img, dirpath):
    if not os.path.isdir(dirpath):
        print('    \033[93mDirectory is not present. Creating a new one.\033[0m\n')
        os.mkdir(dirpath)
    try:
        filename = img.split("/")[-1]
        ext = img.split(".")[-1]
        response = requests.get(img, allow_redirects=True)
        # check if file exists
        if not exists(dirpath+filename):
            # check if name is to long and replace for a shorter random number
            if len(filename) > 128:
               filename = str((random.randrange(100000, 9999999, 1)))+'.'+ext
            if ext == "pdf" or ext == "docx":
                media = "    \033[91m["+ ext +"] \033[0m " + filename + " "
            else:
               media = "    \033[96m["+ ext +"] \033[0m " + filename + " "
            print(media)
            file = open((dirpath+filename), 'wb')
            file.write(response.content)
            file.close()
        else:
            pass
            #print("    \033[91m\u2716\033[0m " + filename + " \033[91mFile Exists!\033[0m")
    except requests.exceptions.RequestException as e:
        error_log(e)
        pass

def find_links(crawl, i, dirpath, download):
    i = int(i)
    to_set = set()
    clean_set = set()
    history = set()
    l = 1
    h = 0

    while i > 0:
        print("\n\033[93mLEVEL: ", l , "\033[0m")
        # get all the working links
        for lines in crawl:
            if lines in history:
                continue
            else:
                h += 1 
                print('\n  \033[1;32m[' , str(h) , '] \033[0m - ',lines , ' \n ')
                if download:
                    crawl_files(lines, dirpath)
            if i > 1:
                try:
                    # save lines in the respond
                    r = session.get(lines)
                    # loop over the found links
                    for urls in r.html.links:
                        # reduce redundancy
                        clean_set.add(urls)
                    for url in clean_set:
                        # animation
                        for j in range(1,8):
                            print('Scraping... \033[1m\033[1;32m'+animation[j]+'\033[0m\033[0m', end='\r')
                            sleep(0.01)
                        # replace spaces
                        url = url.replace(" ", "%20")
                        url_raw = urlparse(url)
                        # get relative path URL's
                        uri = lines.split('/')[:3]
                        if not url_raw.scheme:
                            if url_raw.path.startswith('/'):
                                url = ("/".join(uri))+url_raw.path
                            else:
                                url = ("/".join(uri))+"/"+url_raw.path
                        else:
                            url = url
                        if url.startswith('http'):
                            if not url in to_set:
                                # save in a set to remove duplicates...if any
                                to_set.add(url)
                                crawl = list(to_set)
                    print("                                            ", end="\r")
                except requests.exceptions.RequestException as e:
                    error_log(e)
                    continue
        history.add(lines)
        i = i -1
        l += 1
        h = 0

def crawl_files(urls, dirpath):
    im_set = set()
    # get all the working links for the images 
    try:
        response = urllib.request.urlopen(urls)
        # get the ltd to join with relative path images
        proto = urls.split('/')[:1]
        uri_img = urls.split('/')[:3]
        # filter the files with extensions in the list
        for images in re.findall(':?([^"]*\.(?:png|jpeg|jpg|gif|bmp|pdf|docx))', str(response.read())):
            # animation
            for j in range(1,8):
                print('Looking for files... \033[1m\033[91m'+animation[j]+'\033[0m\033[0m', end='\r')
                sleep(0.01)
            if images[0] == ('/'):
                if images.startswith('//'):
                    images = str(proto[0])+images
                else:
                   images = "/".join(uri_img)+images
            elif images.startswith('http'):
                images = images
            else:
                pass
            if not images in im_set:
                im_set.add(images)
                save_img(images, dirpath)

    except urllib.error.HTTPError as e:
        error_log(e)
        pass
    except urllib.error.URLError as e:
        error_log(e)
        pass
    print("                      ", end="\r")
        
# pretty script end after ctrl + c

def exit_gracefully():
    print('\n\033[92m  ------------------------ @@@ Spider Ends Succesfully!!! @@@ ------------------------\033[0m\n')

def kill_key():
    print('\n\033[91m  -------------------------- *** Keyboard Interrupted!!! *** -------------------------\033[0m', end="\r")
    sleep(0.5)
    print("                                                                                                          ", end="\r")

# collect arguments:
# -r = True/False
# -l = deepness level (default 5)
# -p = path (default "./data/")

def main():
    parser = argparse.ArgumentParser()
    parser.version = '1.0.1'
    parser.add_argument('URL', type=str, help='URL is required')
    parser.add_argument('-r', action='store_true', help='download recursively all the images from the given URL')
    parser.add_argument('-l', default = 5, help='set level of recursion')
    parser.add_argument('-p',default = './data/', help='decide the path to be save downloaded files')

    args = parser.parse_args()
    domain = args.URL
    dirpath = args.p
    download = args.r

    if not domain.startswith('http'):
        domain = 'http://'+domain

    if args.l:
        if int(args.l) > 5:
            i = 5
        else:
            i = args.l
    else:
        i = 5

    if uri_validator(domain) != False:

        domain = requests.get(domain).url

        print("\n \033[93m---------------------------------------------------------------- \033[0m ")
        print (" URL:\t\033[1m"+ domain + "\033[0m")
        print (" Deep:\t\033[1m"+ str(i) + "\033[0m")
        print (" Path:\t\033[1m"+ dirpath + "\033[0m")
        print (" Down:\t\033[1m"+ str(download) + "\033[0m")
        print(" \033[93m---------------------------------------------------------------- \033[0m \n")

        find_links([domain], i, dirpath, download)
    else:
        print('\n\033[91m ',domain, ':\033[0m Invalid URL, try again!\n')

if __name__=="__main__":
    try:
        main()
    except KeyboardInterrupt:
        kill_key()
    finally:
        exit_gracefully()