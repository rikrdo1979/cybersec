#!/usr/bin/env python
# -*- coding: utf-8 -*-

__author__ = "rikrdo"
__copyright__ = "Copyright 2022, Bootcamp Cybersec "
__credits__ = ["rikrdo"]
__license__ = "GPL"
__version__ = "1.0.1"
__maintainer__ = "rikrdo"
__email__ = "rikrdo@rikrdo.es"
__status__ = "Production"

# required libraries

import argparse, requests, re, os, urllib
from tqdm import tqdm
from requests_html import HTMLSession
from time import sleep

session = HTMLSession()

headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:82.0) Gecko/20100101 Firefox/82.0' } 

# save given image/file to given src

def save_img(src, dirpath):
    if not os.path.isdir(dirpath):
        print('\n\033[93mThe directory is not present. Creating a new one.\033[0m')
        os.mkdir(dirpath)
    else:
        print('\n\033[93mThe directory is present.\033[0m')
    for img in src:
        try:
            filename = img.split("/")[-1]
            ext = img.split(".")[-1]
            if ext == "pdf" or ext == "docx":
                print("\n\033[91m["+ ext +"] \033[0m " + filename + "\n")
            else:
                print("\n\033[96m["+ ext +"] \033[0m " + filename + "\n")
            response = requests.get(img, headers=headers, verify='certs.pem', stream = True)
            response.raw.decode_content = True
            for i in tqdm(range(0, 100)):
                with open(f'{dirpath}{filename}', 'wb') as imgfile:
                    imgfile.write(response.content)
                sleep(.005)
        except requests.exceptions.RequestException as e:
            print("\033[91m[error] \033[0m ", e.__dict__)

# save sanitized href to a file

def save_html(crawl, i, dirpath):
    filename = "href"+str(i)+".txt"
    for url in crawl:
        with open(f'{dirpath}{filename}', 'a') as linkfile:
            linkfile.write(url+"\n")
    print("\n\033[93mUrl's & Pictures Saved \033[0m\n")

# find links depending given level

def find_links(crawl, i, dirpath):
    i = int(i)
    proto = crawl[0].split("/")[0]
    to_set = set()
    im_set = set()
    im_list = []

    while i > 0:
        for lines in crawl:
            try:
                r = session.get(lines)
                for url in r.html.links:
                    url = url.replace(" ", "%20")
                    uri = lines.split('/')[2]
                    if url[0] == '/':
                        url = proto + "//" + uri + url
                    elif url.startswith(proto):
                        url = url
                    else:
                        continue
                    print("\033[1;32m[url] \033[0m " + url)
                    to_set.add(url)
                    crawl = list(to_set)
            except requests.exceptions.RequestException as e:
                #print("\033[91m[error] \033[0m ", e.__dict__)
                continue
            try:
                response = urllib.request.urlopen(lines)
                for images in re.findall('(http)?s?:?(\/\/[^"]*\.(?:png|jpeg|jpg|gif|bmp|pdf|docx))', str(response.read())):
                    images = 'https:' + images[1]
                    im_set.add(images)
                    im_list = list(im_set)
            except urllib.error.HTTPError as e:
                #print("\033[91m[error] \033[0m ", e.__dict__)
                continue
            except urllib.error.URLError as e:
                #print("\033[91m[error] \033[0m ", e.__dict__)
                continue
        save_img(im_list, dirpath)
        save_html(crawl, i, dirpath)
        i = i -1
    else:
        return(crawl)

# pretty script end after ctrl + c

def exit_gracefully():
    print('\033[91m\n\n------------------------\n\n   @@@ Bye Bye!!! @@@\n\n------------------------\033[0m')

# collect arguments:
# -r = url ("http://www.example.com/")
# -l = deepness level (default 5)
# -p = path (default "./data/")

def main():
    parser = argparse.ArgumentParser()
    parser.version = '1.0.1'
    parser.add_argument('-r', default ='https://www.42malaga.com/', help='set the url to scrap')
    parser.add_argument('-l', default = 5, help='set level of recursion')
    parser.add_argument('-p',default = './data/', help='decide the path to be save downloaded files')

    args = parser.parse_args()
    
    domain = args.r
    dirpath = args.p

    if args.l:
        if int(args.l) > 5:
            i = 5
        else:
            i = args.l
    else:
        i = 5

    print("\n\033[93m ---------------------------------------------------------------- \033[0m\n")
    print (" URL:\t\033[1m"+ domain + "\033[0m")
    print (" Deep:\t\033[1m"+ str(i) + "\033[0m")
    print (" Path:\t\033[1m"+ dirpath + "\033[0m")
    print("\n\033[93m ---------------------------------------------------------------- \033[0m\n")
    find_links([domain], i, dirpath)

if __name__=="__main__":
    try:
        main()
    except KeyboardInterrupt:
        pass
    finally:
        exit_gracefully()
